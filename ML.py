# -*- coding: utf-8 -*-
"""ML.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1m-xTQJQ_tomIgH6mdxk_DwoJe00-NpA4

#### Importing Modules and Dataset & Renaming Columns to an Understandable Format
"""

#basics
import pandas as pd
import numpy as np

#visualization
from matplotlib import pyplot as plt
import seaborn as sns 

#preprocessing 
from sklearn.preprocessing import StandardScaler

#modeling & model evaluation
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV


import pickle

#Loading the dataset

df = pd.read_csv('df.csv', delimiter =';')

# Dataset before cleaning 

df.shape

# Renaming columns 

df.rename(columns={'y': 'Subscribed?', 'housing': 'Housing_Loan','loan':'Personal_Loan','month':'LastContact_Month','duration':'LastContact_Duration','day':'LastContact_Day','balance':'Average_Balance','marital':'Marital_Status','age':'Age','duration':'LastContact_Duration','education':'Education_Level','job':'Job_Type','poutcome':'PCampaign_Outcome','default':'Credit_in_Default','contact':'Communication_Type','campaign':'PCampaign_TimesContacted','pdays':'Days_Since_LastContact_BeforeCampaign','previous':'BeforeCampaign_TimesContacted'}, inplace=True)

df.head()

df.columns

# Checking data types

df.dtypes

# Checking for null values

df.isnull().sum().sort_values(ascending=False)

#Removing spotted outlier

df = df[df['BeforeCampaign_TimesContacted'] != 275]

"""#### Feature Extraction

First, I created a new categorical variable called **Wealth Bracket** which bins customers into wealth segments based on the average balance of their bank accounts
"""

df['Wealth_Bracket'] = np.nan

# code adopted from : https://towardsdatascience.com/end-to-end-data-science-project-predicting-used-car-prices-using-regression-1b12386c69c8 

#customers with an average balance between negative and 0 will be set as Lowest
m1 = (df['Average_Balance'] <= 0)

#customers with an average balance between 0 and 2k will be set as Lower Middle
m2 = (df['Average_Balance'] > 0) & (df['Average_Balance'] < 2000)

#customers with an average balance between 2k and 5k will be set as Middle
m3 = (df['Average_Balance'] >= 2000) & (df['Average_Balance'] <= 5000)

#customers with an average balance between 5k and 10k will be set as Upper Middle
m4 = (df['Average_Balance'] > 5000) & (df['Average_Balance'] <= 10000)

#customers with an average balance 10k or greater will be set as Highest
m5 = (df['Average_Balance'] > 10000)

df.loc[m1,'Wealth_Bracket'] = df.loc[m1,'Wealth_Bracket'].fillna("Lowest")
df.loc[m2,'Wealth_Bracket'] = df.loc[m2,'Wealth_Bracket'].fillna("Lower Middle")
df.loc[m3,'Wealth_Bracket'] = df.loc[m3,'Wealth_Bracket'].fillna("Middle")
df.loc[m4,'Wealth_Bracket'] = df.loc[m4,'Wealth_Bracket'].fillna("Upper Middle")
df.loc[m5,'Wealth_Bracket'] = df.loc[m5,'Wealth_Bracket'].fillna("Highest")

df['Wealth_Bracket'].head()

"""Then, I created a column called **LastContact_Period** which will have Beginning of Month, Mid-Month, End of Month. This is because people may be less inclined to sign up for the deposit at the end of the month when their salaries have been exhausted. It will replace the Last Contact day which has too many categories."""

df['LastContact_Period'] = np.nan

#If the contact day is between the first and 10th of the month, 'Beginning of Month'
m6 = (df['LastContact_Day'] >= 1) & (df['LastContact_Day'] < 10)

#If the contact day is between the first and 10th of the month, 'Mid-Month'
m7 = (df['LastContact_Day'] >= 10) & (df['LastContact_Day'] < 20)

#If the contact day is greater than 20th of the month, 'End of Month'
m8 = (df['LastContact_Day'] >= 20)

df.loc[m6,'LastContact_Period'] = df.loc[m6,'LastContact_Period'].fillna("Beginning_of_Month")
df.loc[m7,'LastContact_Period'] = df.loc[m7,'LastContact_Period'].fillna("Mid_Month")
df.loc[m8,'LastContact_Period'] = df.loc[m8,'LastContact_Period'].fillna("End_of_Month")

"""Creating a column with age segments, mainly for visualization purposes"""

df['Age_Group'] = np.nan

#'Children'
m9 = (df['Age'] <= 15.0)

#'Youth'
m10 = (df['Age'] > 15.0) & (df['Age'] <= 24.0)

#'Adults'
m11 = (df['Age'] >= 25.0) & (df['Age'] <= 64.0)

#'Seniors'
m12 = (df['Age'] > 64.0)

df.loc[m9,'Age_Group'] = df.loc[m9,'Age_Group'].fillna("Children")
df.loc[m10,'Age_Group'] = df.loc[m10,'Age_Group'].fillna("Youth")
df.loc[m11,'Age_Group'] = df.loc[m11,'Age_Group'].fillna("Adults")
df.loc[m12,'Age_Group'] = df.loc[m12,'Age_Group'].fillna("Seniors")

"""**Reducing the categories** in the Job Type and Campaign Outcome columns """

df['PCampaign_Outcome'] = df['PCampaign_Outcome'].replace('other','unknown')

df['Job_Type'] = df['Job_Type'].replace('technician','blue-collar')
df['Job_Type'] = df['Job_Type'].replace('blue-collar','blue_collar')
df['Job_Type'] = df['Job_Type'].replace('housemaid','services')
df['Job_Type'] = df['Job_Type'].replace('management','white-collar')
df['Job_Type'] = df['Job_Type'].replace('admin.','white-collar')
df['Job_Type'] = df['Job_Type'].replace('white-collar','white_collar')
df['Job_Type'] = df['Job_Type'].replace('entrepreneur','self-employed')
df['Job_Type'] = df['Job_Type'].replace('self-employed','self_employed')

# This code shows the remaining number of categories of each categorical variable

categorical_features=[feature for feature in df.columns if ((df[feature].dtypes=='O') & (feature not in ['Subscribed?']))]
categorical_features

for feature in categorical_features:
    print('The feature is {} and number of categories are {}'.format(feature,len(df[feature].unique())))

df_viz = df

df_viz.head()

#df_viz.to_csv('df_viz.csv') this dataset will be used for visualizations on the streamlit page

"""####One-Hot and Label Encoding """

#Label encoding some of the columns  

df['Wealth_Bracket'] = df['Wealth_Bracket'].map({'Lowest':0,'Lower Middle':1, 'Middle':2, 'Upper Middle':3,'Highest':4})
df['Personal_Loan'] = df['Personal_Loan'].map({'no':0,'yes':1})
df['Credit_in_Default'] = df['Credit_in_Default'].map({'no':0,'yes':1})
df['Housing_Loan'] = df['Housing_Loan'].map({'no':0,'yes':1})
df['Education_Level'] = df['Education_Level'].map({'unknown':0,'primary':1,'secondary':2,'tertiary':3})

#Label encoding the target variable 

df['Subscribed?'] = df['Subscribed?'].map({'no':0,'yes':1})

# Saving the remaining variables in a separate df to be one hot encoded 

df_onehot = ['LastContact_Period', 'PCampaign_Outcome','Marital_Status','LastContact_Month','Job_Type']

df = pd.get_dummies(df, columns=df_onehot)

df.head()

# Dropping Unneeded Columns

df = df.drop(['LastContact_Day'], axis=1)

df.shape

df.head()

"""####Splitting the dataset

Splitting the dataset before scaling the numeric data
"""

X = df.drop(["Subscribed?","LastContact_Duration","Age_Group","Communication_Type"], axis=1) # drop labels for training set, and LastCOntact Duration was dropped because it makes y too obvious
y = df["Subscribed?"].copy()

X_train, X_test, y_train, y_test = train_test_split(X,y,stratify=df['Subscribed?'],test_size=0.20, random_state=42)

X_train.shape, X_test.shape

X_train.head()

X_train.columns

X_train.shape

"""####Models"""

#Defining a function that will be used to show the gridsearch results including the best parameters and accuracy of the best model

def print_gridsearch(grid_search_model, model_string):
  rows=[]
  best_score = grid_search_model.best_score_
  print('Accuracy of Best Model: ', best_score)

  cv_results = grid_search_model.cv_results_
  for acc in cv_results["mean_test_score"]:
    rows.append(acc)

  acc = pd. DataFrame(rows, columns=["accuracy"])
  
  print('Average accuracy for', model_string, 'is:', acc.mean() )
  print('Standard deviation accuracy for', model_string, 'is:', acc.std() )
  print('Best Parameters for', model_string,'are:',grid_search_model.best_params_) #printing this out to see if i want to modify anything to the parameter grid

"""#####Logistic Regression"""

LR_param_grid = {
    "C":[0.01,1,10,100],
    'solver':['liblinear','sag'],
    'penalty' : ['l1', 'l2']

}

#[0.001,0.01,0.1,1,10,100]
LR = LogisticRegression(random_state=42)

LR_cv= GridSearchCV(LR,LR_param_grid,cv=5,n_jobs= -1)

LR_cv.fit(X_train, y_train)

print_gridsearch(LR_cv, 'Logistic Regression')

"""##### Decision Tree Classifier"""

DT_param_grid = {
    'criterion' : ['gini', 'entropy'],
    'splitter': ['best', 'random'],
    'min_samples_split':[2,4,6],
    'max_depth' : list(range(2, 15))
}

DT = DecisionTreeClassifier(random_state= 42)

DT_cv= GridSearchCV(DT,DT_param_grid,cv=5,n_jobs= -1)

DT_cv.fit(X_train,y_train)

print_gridsearch(DT_cv, 'Decision Tree Classifier')

pickle_out = open("finalclassifier.pkl","wb")
pickle.dump(DT_cv, pickle_out)
pickle_out.close()

"""####Scaling Numerical Data"""

num_col_names = ['Age','Average_Balance','PCampaign_TimesContacted','Days_Since_LastContact_BeforeCampaign','BeforeCampaign_TimesContacted']

features = X_train[num_col_names]
scaler = StandardScaler().fit(features.values)
features = scaler.transform(features.values)

features_df = pd.DataFrame(features,columns=num_col_names)
features_df.head()

#X_num.describe()

#X_num.hist(bins=50, figsize=(20,15))
#plt.show()

X_train[num_col_names] = features
X_train.head()

"""#### SVM"""

SVM_param_grid = {
    "C": [0.001, 1],
    "kernel": ["linear","rbf"] 
}


SVM = SVC(random_state= 42)

SVM_cv= GridSearchCV(SVM ,SVM_param_grid, cv=5, n_jobs= -1)

SVM_cv.fit(X_train, y_train)

print_gridsearch(SVM_cv, 'Support Vector Machine')

"""####Feature Importance"""

DT_feature_importance = DT_cv.best_estimator_.feature_importances_
DT_feature_names = X_train.columns.values

#Code adopted from : https://www.analyseup.com/learn-python-for-data-science/python-random-forest-feature-importance-plot.html

def plot_feature_importance(importance,names,model_type):

#Create arrays from feature importance and feature names
  feature_importance = np.array(importance)
  feature_names = np.array(names)

#Create a DataFrame using a Dictionary
  data={'feature_names':feature_names,'feature_importance':feature_importance}
  fi_df = pd.DataFrame(data)

#Sort the DataFrame in order decreasing feature importance
  fi_df.sort_values(by=['feature_importance'], ascending=False,inplace=True)

#Define size of bar plot
  plt.figure(figsize=(10,8))
#Plot Searborn bar chart
  sns.barplot(x=fi_df['feature_importance'], y=fi_df['feature_names'])
#Add chart labels
  plt.title(model_type + 'FEATURE IMPORTANCE')
  plt.xlabel('FEATURE IMPORTANCE')
  plt.ylabel('FEATURE NAMES')

plot_feature_importance(DT_feature_importance,DT_feature_names,'DECISION TREE CLASSIFIER ')

"""#### Performance Evaluation of the best model on test set """

DT_best = DecisionTreeClassifier(criterion= 'gini', max_depth= 4, min_samples_split= 2, splitter= 'random',random_state= 42)
best_model = DT_best.fit(X_train,y_train)

best_model_scores = best_model.score(X_test,y_test)
print('Test Accuracy:', best_model_scores)
